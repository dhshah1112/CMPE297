{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNKn+SDjEI6u6x3dqP83jGz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zOxYrD_ViQKV"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"2eSvM9zX_2d3","executionInfo":{"status":"ok","timestamp":1732928302775,"user_tz":480,"elapsed":55824,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"outputs":[],"source":["%%capture\n","!pip install unsloth\n","# Also get the latest nightly Unsloth!\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""]},{"cell_type":"code","execution_count":2,"metadata":{"id":"QmUBVEnvCDJv","colab":{"base_uri":"https://localhost:8080/","height":382},"outputId":"123713e4-edf9-436f-dace-9dd590fc0a17","executionInfo":{"status":"error","timestamp":1732928316450,"user_tz":480,"elapsed":7532,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-31b79e6d076a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m \u001b[0;31m# Choose any! We auto support RoPE Scaling internally!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m# None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mload_in_4bit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;31m# Use 4bit quantization to reduce memory usage. Can be False.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m# Torch 2.4 has including_emulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mmajor_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminor_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_capability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0mSUPPORTS_BFLOAT16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmajor_version\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mminor\u001b[0m \u001b[0mcuda\u001b[0m \u001b[0mcapability\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \"\"\"\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0mprop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \"\"\"\n\u001b[0;32m--> 523\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"]}],"source":["from unsloth import FastLanguageModel\n","import torch\n","max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/gemma-2-2b\", # using gemma-2b for faster training\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bZsfBuZDeCL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b2f4f348-2228-467b-9288-3bc2ebc586c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unsloth: Offloading input_embeddings to disk to save VRAM\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/unsloth/models/_utils.py:887: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  offloaded_W = torch.load(filename, map_location = \"cpu\", mmap = True)\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Offloading output_embeddings to disk to save VRAM\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth 2024.9.post4 patched 26 layers with 26 QKV layers, 26 O layers and 26 MLP layers.\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Casting embed_tokens to float32\n","Unsloth: Casting lm_head to float32\n"]}],"source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",\n","                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n","    lora_alpha = 32,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = True,   # We support rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n",")"]},{"cell_type":"markdown","metadata":{"id":"vITh0KVJ10qX"},"source":["### Data Prep"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjY75GoYUCB8"},"outputs":[],"source":["# Wikipedia prompt in German\n","wikipedia_prompt = \"\"\"Wikipedia-Artikel\n","### Titel: {}\n","\n","### Artikel:\n","{}\"\"\"\n","\n","EOS_TOKEN = tokenizer.eos_token  # Muss EOS_TOKEN hinzufügen\n","def formatting_prompts_func(examples):\n","    titles = examples[\"title\"]\n","    texts  = examples[\"text\"]\n","    outputs = []\n","    for title, text in zip(titles, texts):\n","        # Muss EOS_TOKEN hinzufügen, sonst läuft die Generierung endlos weiter!\n","        text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n","        outputs.append(text)\n","    return { \"text\" : outputs, }\n","pass\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EsKrpkza3VB3","colab":{"base_uri":"https://localhost:8080/","height":529,"referenced_widgets":["30322d1849fe4cce80685efcb06e2037","92472a9134f94af58a5fc4d52ed48d3f","22045cea66c84bf2a7814c4715c122e2","9d90633f046245dd9a059da79dc1d628","ea50c3c4dcc64cb69ef001d5491f3013","0353a06dbc134eeca47c932a90614951","b6f62a1b9eba424ba88e76ae3ba9f3c5","9755303bd40a4745bcca745d0bc8bd9c","98669a5ff1344bda8ac76e6da1368142","f14786d5549c473f96e31adcbf3a8a27","2953a94f300b43deab169fa0d06a7406","a99ca7fe27c84469b4a4506cd9e7b316","34f5e0033aff4867b63a7b1ac7b56283","05852fecfac447928b4c69a7a06f2944","9e0672a3d7db46ec95856dc908a09448","95d78f2db6be4e1fa0ed4ced8ff7a860","92e944c198f644e8b12742377b06bc11","9ee14602d89743b7bf9e4d231457f9de","76a116ec1c9047a8a8db78d24d540823","2f94638c1a124e9d8c72ee0c1560f0a6","b94f8eb80c584f43b5b0a5f7876ad207","1cc1b0a49ae243ef96c95d58eb49b4ae","62b27c305bcc499e8a1596e992d3d07a","100c7dd5c44e4b01b2d43141fd3cc19b","f31f09edbc034d498d12cd3e6df399b0","3e7e06a13e194affb18cf3d583ff0c5b","27583e8c3ed64e3fa9771529bda06556","feb72da29cd8451789ce958916ae471e","d2c8ecf104704af58ea9479d49493f09","a7fcfaf75a1e4d48864142f19599a984","edbc18ab5e824c3faa11327848eb0b34","3d40d0bd353a49bc81b57af1da01b712","7b75942cd0d443ecb7820c3c4835bdaf","c9e3053c346e4e9da7dc4486ce96e12b","d2a0be40a9e84b89bc4749f2be21dc40","3292d986aa7f46ccbec6af1eac1e39b1","c7fe885788ba454982d25adb3ac8eca9","0a8bcd4dd81d4914937e318f76d45e18","f9c1d9b837ac4de582ac56711f84eca3","1171302554c9417691c49633a2b7ca38","9c7c5feab4834f1b9fb26454de781386","09d63d5ffd35434ca0e84f278c7e4cc8","2df819ef67954fe89ff734ec80c46ea3","935de51da45c43f6818680f9afdae7fd","32af8272e3974c96879887cbf8ad40a3","c85f9bb77ce845b798deebc8dac8ce5a","a9f15b487c0641ffac1abf1a81d2d046","02dce1afe132457d8104c50fd50eace5","4f35f28d5d054c09b142c653ae1218a2","d5f5ed379e724ef5a756ffd636c66a5e","fe69a5ed74ae43cd9b79c90225423f04","7b05f2c6ed7f4115ac65f3401aff7e5a","3d80f71be24649fb8fc60103eb561dc0","acfa90c864794a3988097bf86b46d91a","2d278fe6cfc443cc9e0f697f4fd18e02","2a09a975209341668de3bc919f687df6","b1221a63f17642e48f4cfad0156ea999","3f3983e052d9439bbc33d62f43691f0b","e36e794c2e6e44e594cb16caadac6df0","86437510400948569e70c7c5fd87ebc0","fd4a5dd11f424b83942c3d02e73f736f","6e966fc6c4cb44fbbae32e0d95db10c2","a6223af1f9d142129b325955dd3006a0","14e3451c7ffd4102974b4655fb41e814","ae5d4b4871f5432cbb263d0cf3eb0d00","9b4dec2441ae43298c3283d4ebb1e61f","b79a33822377482fba0c5e6701de8e71","f84623211fcd49cb91ade9e7a4feb9c3","6aba123ada5b4c6e8dfca380bde4d820","a3b98c222abb4ef7a0f2f21daa3e402f","91b97bac16f0453c96512a51cb18246e","cb780bc5712c4d1e91c95770d05f1c87","40944c2907c244a9bc7805ea3a5f6091","6f2043346af941e7a4c958b4f14513e9","b0d473580c6c4514a56f4b82502322c9","eec04a47f2e9419fb1a6b90626cd4a27","ac42fe25681e48818e7baaade4f4e5d9","50d5d10a0ce64056839e8bf450d5ee2a","e1ee82407949414399a87503a42a10e8","9d499db6515e463ba7fefbbbfd6b5946","c0ef5d846f1647f2a0f2abd12127b9b0","ee74ae947e84480bbc7a8bee7b3f85e2","627f70e7558b4b82a011c2dc9441abcc","00615267f4654270a70aa6f083143028","2c5b0988ecc24cecbacede76f76f946f","80225227582e46229f47ce68483ad210","7e06e7af9fcd4b268b2d6760b414ed3c","992647fd4b1e424287b75f22eb927d56","ba143a5998eb487b806707d9f16df7f7","764402d7165c4804b6361407867e423f","9d0d2f65de3c4ec688c722001b0f2cf8","cf6741e80d09410bab934a52f5908f06","3b4b6bea2dee46c58f5ef7660cff63b3","e234a362489b48d6a1340f536af3946d","475de621b9dd450189f832e183751164","8b72b6ba8bf64aeda83206d4de0bee26","c47b251be7bd485eaf9fbaf739e550e1","ffd2cb8367c0405e88660c9a5a45d7e9","fc498e5ee47b48558a082d184a63b7ea","367d298765ab453d8cb1590ddf68ad77","265f032924e546199e68cbb9be5d536f","ef73b410c2f640cbba26230a79b49e19","055da939d686412a90367c5df0d19c69","ad48d6e8302942aab4067b7065e231dc","4feedeb39df54aeebf139371310693d8","b71b518c51824c0b9a296509be0f9e37","ff42295c9d0c4b8f9a68fbc238ede68b","026ec8738055487bba3a27fe55d5c3bc","263937a6ed7448f0976f919d3ec0cf02","943a31b7683f4a598eed2a0e5c1c0382","be307b82a83344d3a809062f34920b5a","6e5fbe44d3cb4d3bbcc413b8f5d81c0a","e49fd219f2c1484dbbe5d5c1405e5dd4","8c384087c25144a79f9047a2a32ae236","8481c12330f841c89c16b96f26f5dd4b","dbf38f36b4d9424599e384adaa8daf8a","ff5d11098f764558bd2bb62ccc5c5f00","88ee187e38814b5db56ef893e2c42fa0","555ea318dff445e09895255c9dd421b6","f9688e0e0830470fab60334046d270e9","d4e5fa190b61400c99c34302bab425c9","04cdb2966f4b483b8ecec2f0d6cd544b","bef28415fe354bb49bb022018f9a8e8c","d77e13cbdbb64d6ba55c1a48839ca6d9","0d6a917eabe641a9a1c3af194f62588c","e7701fb3806949e98f03602734d04e99","4febbd73ce4847088c448eda31e6881e","a67f73a68dec44238f44af3f3e622719","5711aca49ae14257b4026381fccac035","aab18d11e49e4dd2afb22bfe622e5b35","bf32975b11d44f618a474a9f11262b64","92b92f05641b4d878d11a734b62715bc","d830ef9ebc0d494986e277d4f4ba6dcd","ea8e27dad3d740a591a87a4d04537594","bbe1443959db4aef8a6e648c910cf565","5fc342d1d61643b495972e5e41c7b338","29682ed1e1e2472f945609681bbdeddc","97dc966beb46442a88da71e8bf50c197","cb98b184faf540c5be8523c37cb6d970","936fb9980ef649b88764632a27ec9bee","e144a75ff1254e32b57ece2245e9a784","1af233a93e9642feb1f8a3431a21504a","74f586b052674f4ab3c886e226dda002","88291238fb5246b8b83868be9f00d68f","f89cf1ae1a2a42718a2a4b1730b33c0f","081564bd303a4f61ad97abf1cbce6978","681370e21bea46f19067ddb906dfe3cc","d64358a369104e9eb3349e18bd0ef11a","fc3f4b6652d04f6b8c1e004812a2bc5c","ac6970b9adfa4d02b316f28ed10917b5","9d340290cb41477e922be1664a006276","86db08707c1740509fdf5bca71e6fbbb","0c9fed82f9784d49ac8d9a1b01b8ff40","da1d63825a6a42eebff9d385b24703cc","6782cce9b4774845ad3ec5a107c46094","f5976fa1d289443fb4828077f322bd5c","338d9b71141146feac1ecd604c119064","b0521957c8a24868b6f059ba071cfcef","cc2fe410fd9c479ba889a8aac1f575ed","d8a36bd2255a4103a6b5bcdde56793c4","73f3081bb3b14d878d504548e5d98f73","215caf1f14454b7d9ea42d0ee5d926a8","7026866676374655adad4d2bb33e5043","1368a3f191344e66bb6c6b886b46c6d2","920c6f3f9d914868941377875c74ce64","2c05207943964589831b7f230400c87c","c787693ac2634290927f77ceab35a152","33f96743668241268d0858addd5add26","79b51d75221a422f84b07593bb9c8d8c","1ffb14aacdcc4250993f7883fefa8d67","f9aa2d24fc7c405daca2bd6f328448cc","2618558095204b7bbdc688d491eba515","0f7467bb0f1f4d21940aa545e81feaa6","fdf49e600f944ebcae4db1f39e90e4cf","4f21b303b17b4240b09e05c3d33b7b49","9f4f22697a584edeac691eee6c41820f"]},"outputId":"5f0ff73c-7216-4eab-95b6-086bf512481d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/131k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30322d1849fe4cce80685efcb06e2037"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00000-of-00013.parquet:   0%|          | 0.00/688M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a99ca7fe27c84469b4a4506cd9e7b316"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00001-of-00013.parquet:   0%|          | 0.00/376M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62b27c305bcc499e8a1596e992d3d07a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00002-of-00013.parquet:   0%|          | 0.00/287M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9e3053c346e4e9da7dc4486ce96e12b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00003-of-00013.parquet:   0%|          | 0.00/245M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32af8272e3974c96879887cbf8ad40a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00004-of-00013.parquet:   0%|          | 0.00/168M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a09a975209341668de3bc919f687df6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00005-of-00013.parquet:   0%|          | 0.00/178M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b79a33822377482fba0c5e6701de8e71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00006-of-00013.parquet:   0%|          | 0.00/216M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50d5d10a0ce64056839e8bf450d5ee2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00007-of-00013.parquet:   0%|          | 0.00/241M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba143a5998eb487b806707d9f16df7f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00008-of-00013.parquet:   0%|          | 0.00/227M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"367d298765ab453d8cb1590ddf68ad77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00009-of-00013.parquet:   0%|          | 0.00/223M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be307b82a83344d3a809062f34920b5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00010-of-00013.parquet:   0%|          | 0.00/167M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04cdb2966f4b483b8ecec2f0d6cd544b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00011-of-00013.parquet:   0%|          | 0.00/254M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d830ef9ebc0d494986e277d4f4ba6dcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00012-of-00013.parquet:   0%|          | 0.00/226M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88291238fb5246b8b83868be9f00d68f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/1841155 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6782cce9b4774845ad3ec5a107c46094"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/18411 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c05207943964589831b7f230400c87c"}},"metadata":{}}],"source":["from datasets import load_dataset\n","\n","# Load the German Wikipedia dataset\n","dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.de\", split=\"train\")\n","\n","# We select 1% of the data to make training faster!\n","dataset = dataset.train_test_split(train_size=0.01)[\"train\"]\n","\n","# Apply the formatting function\n","dataset = dataset.map(formatting_prompts_func, batched=True)\n"]},{"cell_type":"markdown","metadata":{"id":"idAEIeSQ3xdS"},"source":["### Continued Pretraining"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"95_Nn-89DhsL","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["065746dcbe6d4fcb8cd761b608a1fae8","06af23263a7c4dafb770f1d723b43043","2ef5f71f79944e339e40c7d71ca25b66","80c70c2f09784b81832af46f2f639476","c37400c0e6754301a03c93ed89895377","938f8b55f7984ff2a019d3a7ac95d96a","0eb99a5442644a23a356e48b2e58128f","43cecffd275549c2aee3b42467ea5dae","e03330a1bfea416484c4356f4e56a1c9","98d9712ad33e43c482a0951a67287f08","7096351ffb8d48e5ad87530687f0e919"]},"outputId":"11c22433-fa41-4c88-9c76-ba5ac8e98048"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map (num_proc=2):   0%|          | 0/18411 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"065746dcbe6d4fcb8cd761b608a1fae8"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["max_steps is given, it will override any value given in num_train_epochs\n"]}],"source":["from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","from unsloth import UnslothTrainer, UnslothTrainingArguments\n","\n","trainer = UnslothTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","    args = UnslothTrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 8,\n","        max_steps = 120,\n","        warmup_steps = 10,\n","        learning_rate = 5e-5,\n","        embedding_learning_rate = 1e-5,\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","    ),\n",")"]},{"cell_type":"markdown","source":["### Show current memory stats"],"metadata":{"id":"zbis5qPTFo9J"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ejIt2xSNKKp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e641f18e-0240-47bb-fd6a-61d340ad849f"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.564 GB.\n","8.604 GB of memory reserved.\n"]}],"source":["gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqxqAZ7KJ4oL","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"408becb7-eda9-417a-a5b5-aedfdad903d6"},"outputs":[{"output_type":"stream","name":"stderr","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 18,411 | Num Epochs = 1\n","O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 8\n","\\        /    Total batch size = 16 | Total steps = 120\n"," \"-____-\"     Number of trainable parameters = 1,345,781,760\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for embed_tokens.\n","Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for lm_head.\n"]},{"output_type":"stream","name":"stderr","text":["AUTOTUNE bmm(16x648x256, 16x256x648)\n","  bmm 0.0614 ms 100.0%\n","  triton_bmm_10 0.0860 ms 71.4%\n","  triton_bmm_6 0.0891 ms 69.0%\n","  triton_bmm_13 0.0891 ms 69.0%\n","  triton_bmm_9 0.0911 ms 67.4%\n","  triton_bmm_5 0.0932 ms 65.9%\n","  triton_bmm_14 0.0973 ms 63.2%\n","  triton_bmm_15 0.1198 ms 51.3%\n","  triton_bmm_2 0.1265 ms 48.6%\n","  triton_bmm_3 0.1280 ms 48.0%\n","SingleProcess AUTOTUNE benchmarking takes 2.3546 seconds and 0.0117 seconds precompiling\n","AUTOTUNE bmm(16x648x648, 16x648x256)\n","  bmm 0.0410 ms 100.0%\n","  triton_bmm_33 0.0983 ms 41.7%\n","  triton_bmm_29 0.1116 ms 36.7%\n","  triton_bmm_37 0.1167 ms 35.1%\n","  triton_bmm_32 0.1188 ms 34.5%\n","  triton_bmm_24 0.1219 ms 33.6%\n","  triton_bmm_34 0.1270 ms 32.3%\n","  triton_bmm_22 0.1311 ms 31.2%\n","  triton_bmm_25 0.1393 ms 29.4%\n","  triton_bmm_28 0.1485 ms 27.6%\n","SingleProcess AUTOTUNE benchmarking takes 2.3292 seconds and 0.0018 seconds precompiling\n","AUTOTUNE bmm(16x648x648, 16x648x256)\n","  bmm 0.0410 ms 100.0%\n","  triton_bmm_81 0.1280 ms 32.0%\n","  triton_bmm_78 0.1331 ms 30.8%\n","  triton_bmm_76 0.1352 ms 30.3%\n","  triton_bmm_79 0.1413 ms 29.0%\n","  triton_bmm_82 0.1823 ms 22.5%\n","  triton_bmm_85 0.1864 ms 22.0%\n","  triton_bmm_89 0.1915 ms 21.4%\n","  triton_bmm_92 0.2048 ms 20.0%\n","  triton_bmm_83 0.2191 ms 18.7%\n","SingleProcess AUTOTUNE benchmarking takes 2.3673 seconds and 0.0010 seconds precompiling\n","AUTOTUNE bmm(16x256x648, 16x648x648)\n","  bmm 0.0420 ms 100.0%\n","  triton_bmm_119 0.1280 ms 32.8%\n","  triton_bmm_117 0.1331 ms 31.5%\n","  triton_bmm_114 0.1372 ms 30.6%\n","  triton_bmm_116 0.1423 ms 29.5%\n","  triton_bmm_120 0.1864 ms 22.5%\n","  triton_bmm_123 0.1905 ms 22.0%\n","  triton_bmm_127 0.1905 ms 22.0%\n","  triton_bmm_124 0.2263 ms 18.6%\n","  triton_bmm_128 0.2263 ms 18.6%\n","SingleProcess AUTOTUNE benchmarking takes 2.3719 seconds and 0.0013 seconds precompiling\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [120/120 09:17, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.161200</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2.166400</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.994400</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>2.226800</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.885100</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>1.847900</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>1.902200</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>2.071200</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>2.110400</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>2.115700</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>2.144400</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>2.142100</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>1.724300</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>1.904200</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>2.033500</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>1.796100</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>1.601900</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>1.812600</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>1.886900</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.908600</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>1.972300</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>1.999700</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>1.897200</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>2.121100</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>1.831800</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>2.052700</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>2.145700</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>1.972900</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>1.945700</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>2.017400</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>1.894900</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>1.679400</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>1.724300</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>2.007900</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>2.015100</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>1.791000</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>1.727600</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>2.006000</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>1.754700</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>2.083500</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>1.915300</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>1.852300</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>2.024100</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>2.061400</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>2.055800</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>1.790600</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>1.620400</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>1.839900</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>1.748400</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.714900</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>1.917800</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>1.679100</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>1.904600</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>1.819900</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>1.844600</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>1.857800</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>1.823400</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>1.919000</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>1.742000</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>2.158800</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>1.906400</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>1.793300</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>1.668400</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>1.890800</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>2.319500</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>1.829200</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>1.831700</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>2.102600</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>2.185800</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>1.884900</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>1.465300</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>1.725800</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>1.818800</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>1.849400</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>1.896100</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>2.001000</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>1.863600</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>1.994700</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>1.977900</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>2.039800</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>2.005500</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>1.782500</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>2.270600</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>1.989500</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>1.714500</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>1.571500</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>1.778700</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>2.081200</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>1.520100</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>2.044700</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>1.752000</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>1.957900</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>1.814300</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>2.195900</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>1.721700</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>1.852100</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>1.788700</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>1.536600</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>1.836800</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.803400</td>\n","    </tr>\n","    <tr>\n","      <td>101</td>\n","      <td>1.922900</td>\n","    </tr>\n","    <tr>\n","      <td>102</td>\n","      <td>1.717700</td>\n","    </tr>\n","    <tr>\n","      <td>103</td>\n","      <td>1.687900</td>\n","    </tr>\n","    <tr>\n","      <td>104</td>\n","      <td>2.067800</td>\n","    </tr>\n","    <tr>\n","      <td>105</td>\n","      <td>2.080900</td>\n","    </tr>\n","    <tr>\n","      <td>106</td>\n","      <td>1.870400</td>\n","    </tr>\n","    <tr>\n","      <td>107</td>\n","      <td>1.865200</td>\n","    </tr>\n","    <tr>\n","      <td>108</td>\n","      <td>1.714800</td>\n","    </tr>\n","    <tr>\n","      <td>109</td>\n","      <td>1.881200</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>1.538800</td>\n","    </tr>\n","    <tr>\n","      <td>111</td>\n","      <td>1.649700</td>\n","    </tr>\n","    <tr>\n","      <td>112</td>\n","      <td>2.111800</td>\n","    </tr>\n","    <tr>\n","      <td>113</td>\n","      <td>2.072600</td>\n","    </tr>\n","    <tr>\n","      <td>114</td>\n","      <td>1.868700</td>\n","    </tr>\n","    <tr>\n","      <td>115</td>\n","      <td>2.050200</td>\n","    </tr>\n","    <tr>\n","      <td>116</td>\n","      <td>1.676400</td>\n","    </tr>\n","    <tr>\n","      <td>117</td>\n","      <td>1.726000</td>\n","    </tr>\n","    <tr>\n","      <td>118</td>\n","      <td>1.902000</td>\n","    </tr>\n","    <tr>\n","      <td>119</td>\n","      <td>2.047500</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>1.793000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["AUTOTUNE bmm(16x256x256, 16x256x256)\n","  bmm 0.0184 ms 100.0%\n","  triton_bmm_158 0.0215 ms 85.7%\n","  triton_bmm_162 0.0236 ms 78.3%\n","  triton_bmm_157 0.0246 ms 75.0%\n","  triton_bmm_161 0.0246 ms 75.0%\n","  triton_bmm_165 0.0246 ms 75.0%\n","  triton_bmm_167 0.0256 ms 72.0%\n","  triton_bmm_166 0.0266 ms 69.2%\n","  triton_bmm_154 0.0276 ms 66.7%\n","  triton_bmm_155 0.0276 ms 66.7%\n","SingleProcess AUTOTUNE benchmarking takes 2.1826 seconds and 0.0106 seconds precompiling\n","AUTOTUNE bmm(16x256x256, 16x256x256)\n","  bmm 0.0164 ms 100.0%\n","  triton_bmm_185 0.0266 ms 61.5%\n","  triton_bmm_177 0.0276 ms 59.3%\n","  triton_bmm_181 0.0276 ms 59.3%\n","  triton_bmm_176 0.0287 ms 57.1%\n","  triton_bmm_184 0.0297 ms 55.2%\n","  triton_bmm_189 0.0297 ms 55.2%\n","  triton_bmm_174 0.0307 ms 53.3%\n","  triton_bmm_186 0.0307 ms 53.3%\n","  triton_bmm_173 0.0328 ms 50.0%\n","SingleProcess AUTOTUNE benchmarking takes 2.1682 seconds and 0.0018 seconds precompiling\n","AUTOTUNE bmm(16x256x256, 16x256x256)\n","  bmm 0.0164 ms 100.0%\n","  triton_bmm_233 0.0297 ms 55.2%\n","  triton_bmm_228 0.0307 ms 53.3%\n","  triton_bmm_243 0.0328 ms 50.0%\n","  triton_bmm_230 0.0338 ms 48.5%\n","  triton_bmm_231 0.0420 ms 39.0%\n","  triton_bmm_237 0.0440 ms 37.2%\n","  triton_bmm_241 0.0440 ms 37.2%\n","  triton_bmm_242 0.0471 ms 34.8%\n","  triton_bmm_238 0.0481 ms 34.0%\n","SingleProcess AUTOTUNE benchmarking takes 2.1977 seconds and 0.0012 seconds precompiling\n"]}],"source":["trainer_stats = trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"R9dRBJZulavZ"},"source":["### Instruction Finetuning\n","\n","We now use the [Alpaca in GPT4 Dataset](https://huggingface.co/datasets/FreedomIntelligence/alpaca-gpt4-korean) but translated in Korean!\n","\n","Go to [vicgalle/alpaca-gpt4](https://huggingface.co/datasets/vicgalle/alpaca-gpt4) for the original GPT4 dataset for Alpaca or [MultilingualSIFT project](https://github.com/FreedomIntelligence/MultilingualSIFT) for other translations of the Alpaca dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1oNjUwxOyG8C","colab":{"base_uri":"https://localhost:8080/","height":148,"referenced_widgets":["bb273b9ec5f94f88a416a645cd0b23a7","8a12b2356d824a2abc52e9df1c5a8fd3","cede8cf084914888b69d975f2c6f1689","20143f459e64457199ad040e304520f6","1e6d725a804945a08cd103586a940673","4e9e3fa560914ab0bb9f9b9a32189041","e484da3c840949f5bc3581c9b1f26619","57a8ef55b2bc4c01becdadd6ac687252","71f4f17a77fc441ebf202e400a228e9b","808796b9cffc4a2593008a95e2c13c19","2c750434447c477d92f107c30025190a","b2647b5f925f46e088eece3c33174250","99f85e90ab18436ba98963f02b0d5093","96f3bca876ff48ad86d9ee3870eff255","5ad0594aea9b4cdaab6fe9db1137237b","9309ecb64efb4236ba1125f618dcda7e","301e95cfbbf4496f9084c97fce1b354d","21a67b293afa4e4e865acc91eaa2fee3","8600925444714c098d8b399844da1033","d79e73d2ed6044c58568e36cbb6506d1","55f06a59fd1c408a92fd9aa8708c0444","2fda5a414ab143888dcf1d35d36c5d4c","ff42e9cc937647afb021e9f5bfdd7ac1","d7a193fce0584e89a81b143a04368c6a","391d0c0f434a4f9eb7e0a0aa38a86787","33c2358343b043269a5b6f9459f0c0c1","948e21bf607a4003b8b1735d045be166","ddc46679d4e14ae7919b28e6912b3716","be4c855b857f4a6092fc9a8da92dadd6","5c98175bd8d840e78e903d6f03391267","bd114008e9454a859f2639161a73bae9","d26a7e77add14ed6b443b4e1d610e8ad","ea367a5a97774d339db801f2f7027aa4"]},"outputId":"eb852b6f-d054-45ab-b4bd-0cb74b1bb9fc"},"outputs":[{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb273b9ec5f94f88a416a645cd0b23a7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Repo card metadata block was not found. Setting CardData to empty.\n","WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"]},{"output_type":"display_data","data":{"text/plain":["alpaca-gpt4-italian.json:   0%|          | 0.00/51.7M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2647b5f925f46e088eece3c33174250"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/49969 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff42e9cc937647afb021e9f5bfdd7ac1"}},"metadata":{}}],"source":["from datasets import load_dataset\n","alpaca_dataset = load_dataset(\"FreedomIntelligence/alpaca-gpt4-deutsch\", split = \"train\")"]},{"cell_type":"markdown","metadata":{"id":"kmFG41nFytgi"},"source":["We print 1 example:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvTfIKaUQxQ5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"364c9b2b-a33c-443b-cbc4-b1fe2fea9880"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'conversations': [{'from': 'human', 'value': 'Suggerisci uno slogan per una campagna di riciclaggio.\\n'}, {'from': 'gpt', 'value': '1. \"Riduci, riutilizza, ricicla: Insieme per un futuro più verde.\"\\n2. \"Ricicla oggi, per un domani migliore.\"\\n3. \"Trasforma la tua spazzatura in tesoro - Ricicla!\"\\n4. \"Ricicla per il ciclo della vita.\"\\n5. \"Risparmia risorse, ricicla di più.\"'}], 'id': '23712'}\n"]}],"source":["print(alpaca_dataset[0])"]},{"cell_type":"markdown","metadata":{"id":"OO8UY34ql2vJ"},"source":["We again use https://translate.google.com/ to translate the Alpaca format into Korean"]},{"cell_type":"markdown","metadata":{"id":"mxWWh8xsl9XT"},"source":["We again employ `UnslothTrainer` and do instruction finetuning!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Zul21NSRRLP","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["00448c9504634a4d8ddd6666b30e9af2","75966a716f604161a4dcfe4c29bc2a31","19b6862dd39341fe842bedac397ba1e9","5686b1dab04f4d51908e5b5636a2c565","51762d0597f64adda9e42df595a55e3a","f60184f23b2d4466ab9b8ea759b7b242","9dbebed8053a449fba35f0b46d2964b2","388cd57395644bfaabfec80887d09ae3","b60c7ffd69194f0b865592ff2e5d1910","5c4eb11cdfed4753be491eeed23ba70e","6cc97eed168743bcbd843de9e2d53331"]},"outputId":"229aeeba-493d-4e96-e743-05cb5706c649"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map (num_proc=8):   0%|          | 0/49969 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00448c9504634a4d8ddd6666b30e9af2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["max_steps is given, it will override any value given in num_train_epochs\n"]}],"source":["from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","from unsloth import UnslothTrainer, UnslothTrainingArguments\n","\n","trainer = UnslothTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = alpaca_dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 8,\n","\n","    args = UnslothTrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 8,\n","\n","        # Use num_train_epochs and warmup_ratio for longer runs!\n","        max_steps = 120,\n","        warmup_steps = 10,\n","        # warmup_ratio = 0.1,\n","        # num_train_epochs = 1,\n","\n","        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n","        learning_rate = 5e-5,\n","        embedding_learning_rate = 1e-5,\n","\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.00,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","    ),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DIO7c1FoRe-X","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"07f97947-ab1b-4b9a-fcec-2c3ef1434052"},"outputs":[{"output_type":"stream","name":"stderr","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 49,969 | Num Epochs = 1\n","O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 8\n","\\        /    Total batch size = 16 | Total steps = 120\n"," \"-____-\"     Number of trainable parameters = 1,345,781,760\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for embed_tokens.\n","Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for lm_head.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [120/120 07:16, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.210600</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2.151700</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.820900</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.685800</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.525100</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>1.335000</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>1.453000</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>1.430400</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>1.410900</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>1.336000</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>1.306500</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>1.409100</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>1.298800</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>1.335700</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>1.236900</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>1.375400</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>1.235500</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>1.328500</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>1.368700</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.228000</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>1.139200</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>1.225100</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>1.060600</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>1.173500</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>1.363000</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>1.134200</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>1.229400</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>1.164500</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>1.326600</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.143800</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>1.357900</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>1.208800</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>1.294300</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>1.403700</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>1.202200</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>1.327400</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>1.290800</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>1.315700</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>1.123600</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.251100</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>1.192800</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>1.166700</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>1.300600</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>1.385900</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>1.252400</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>1.102200</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>1.162600</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>1.217800</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>1.264100</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.225700</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>1.255900</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>1.050900</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>1.209300</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>1.223600</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>1.191100</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>1.209000</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>1.139500</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>1.165100</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>1.230900</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.237700</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>1.286800</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>1.247700</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>1.202900</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>1.182700</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>1.282800</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>1.170500</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>1.192500</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>1.079200</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>1.223900</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>1.399100</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>1.190500</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>1.208300</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>1.243700</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>1.226700</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>1.273000</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>1.074200</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>1.102100</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>1.296800</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>1.155200</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>1.094900</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>1.159500</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>1.202400</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>1.165900</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>1.022100</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>1.344600</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>1.208900</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>1.286900</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>1.256100</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>1.096700</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>1.178500</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>1.210200</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>1.103900</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>1.199800</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>1.215200</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>1.146300</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>1.117600</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>1.232100</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>1.170800</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>1.163200</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.176400</td>\n","    </tr>\n","    <tr>\n","      <td>101</td>\n","      <td>1.083200</td>\n","    </tr>\n","    <tr>\n","      <td>102</td>\n","      <td>1.149800</td>\n","    </tr>\n","    <tr>\n","      <td>103</td>\n","      <td>1.350000</td>\n","    </tr>\n","    <tr>\n","      <td>104</td>\n","      <td>1.142900</td>\n","    </tr>\n","    <tr>\n","      <td>105</td>\n","      <td>1.035300</td>\n","    </tr>\n","    <tr>\n","      <td>106</td>\n","      <td>1.207800</td>\n","    </tr>\n","    <tr>\n","      <td>107</td>\n","      <td>1.271100</td>\n","    </tr>\n","    <tr>\n","      <td>108</td>\n","      <td>1.225900</td>\n","    </tr>\n","    <tr>\n","      <td>109</td>\n","      <td>1.171800</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>1.197600</td>\n","    </tr>\n","    <tr>\n","      <td>111</td>\n","      <td>1.281200</td>\n","    </tr>\n","    <tr>\n","      <td>112</td>\n","      <td>1.098600</td>\n","    </tr>\n","    <tr>\n","      <td>113</td>\n","      <td>1.304400</td>\n","    </tr>\n","    <tr>\n","      <td>114</td>\n","      <td>1.215000</td>\n","    </tr>\n","    <tr>\n","      <td>115</td>\n","      <td>1.279500</td>\n","    </tr>\n","    <tr>\n","      <td>116</td>\n","      <td>1.172800</td>\n","    </tr>\n","    <tr>\n","      <td>117</td>\n","      <td>1.129100</td>\n","    </tr>\n","    <tr>\n","      <td>118</td>\n","      <td>1.077300</td>\n","    </tr>\n","    <tr>\n","      <td>119</td>\n","      <td>1.229900</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>1.124200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}],"source":["trainer_stats = trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"ekOmTR1hSNcr"},"source":["<a name=\"Inference\"></a>\n","### Inference\n","Let's run the model! You can change the instruction and input - leave the output blank!\n","\n","Remember to use https://translate.google.com/!"]},{"cell_type":"markdown","metadata":{"id":"CrSvZObor0lY"},"source":[" You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2pEuRb1r2Vg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9360f117-f5e6-4f2e-ad68-b223cf35447c"},"outputs":[{"output_type":"stream","name":"stdout","text":["<bos>Di seguito è riportata un'istruzione che descrive un compito. Scrivi una risposta che completi adeguatamente la richiesta.\n","\n","### Istruzione:\n","Com'è la musica latina?\n","\n","### Risposta:\n","La musica latina è una vasta categoria che comprende una varietà di stili musicali che hanno origine in varie parti del mondo latino. Alcuni dei principali stili musicali latini includono la salsa, il merengue, il bachata, il cumbia, il reggaeton e il hip-hop latino. Questi stili sono spesso caratterizzati da ritmi vivaci, melodie accattivanti e una forte base ritmica. La musica latina è spesso associata a una forte componente danzabile, con molti stili che richiedono una certa conoscenza della danza.<eos>\n"]}],"source":["# alpaca_prompt = Copied from above\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    alpaca_prompt.format(\n","\n","        \"Was ist ein Sonnensystem?\", # instruction\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"]}]}