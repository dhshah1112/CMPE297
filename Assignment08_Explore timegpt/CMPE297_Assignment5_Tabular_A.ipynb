{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOe0RaY1fx+g4BERKiTOaHN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"84f6-l8YB-Jn"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["## Assignment Description\n","Create a Colab that generates synthetic data for a real dataset using Tabula. Include explanations for the data generation process and how it compares to the original data.\n","\n","### References\n","1. [Tabula_on_insurance_dataset.ipynb](https://github.com/zhao-zilong/Tabula/blob/main/Tabula_on_insurance_dataset.ipynb)\n"],"metadata":{"id":"AHHSBvsjeHus"}},{"cell_type":"markdown","source":["### Synthetic Data Generation"],"metadata":{"id":"QcQYno-GGN7x"}},{"cell_type":"code","source":["!git clone https://github.com/zhao-zilong/Tabula.git\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EQKCTnFVG-1y","outputId":"b7f7ea49-a20a-4216-c352-706f6fae0cc7","executionInfo":{"status":"ok","timestamp":1733594423103,"user_tz":480,"elapsed":1149,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Tabula'...\n","remote: Enumerating objects: 61, done.\u001b[K\n","remote: Counting objects: 100% (61/61), done.\u001b[K\n","remote: Compressing objects: 100% (44/44), done.\u001b[K\n","remote: Total 61 (delta 30), reused 40 (delta 16), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (61/61), 54.05 KiB | 1.15 MiB/s, done.\n","Resolving deltas: 100% (30/30), done.\n"]}]},{"cell_type":"code","source":["!cd Tabula\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kuIu8GyrHMw1","outputId":"b533f2b3-3108-4023-980b-c6efb72c26a7","executionInfo":{"status":"ok","timestamp":1733594430647,"user_tz":480,"elapsed":345,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["!pip install datasets>=2.5.2\n","!pip install numpy>=1.24.2\n","!pip install tqdm>=4.64.1\n","!pip install transformers>=4.22.1\n","!pip install pandas>=1.4.4\n","!pip install scikit_learn>=1.1.1\n","!pip install torch>=1.10.2"],"metadata":{"id":"2VhcwpX3zYKj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733594474483,"user_tz":480,"elapsed":32361,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}},"outputId":"8e16fa52-a8da-4ae4-8f50-d986fa7ffc0d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","execution_count":4,"metadata":{"scrolled":true,"id":"49c5848f","executionInfo":{"status":"ok","timestamp":1733594475042,"user_tz":480,"elapsed":562,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"outputs":[],"source":["# change tabula to tabula_middle_padding to test middle padding method\n","import pandas as pd"]},{"cell_type":"code","source":["data_url = \"https://raw.githubusercontent.com/zhao-zilong/Tabula/refs/heads/main/Real_Datasets/Insurance_compressed.csv\""],"metadata":{"id":"mE5jEMEd1VvQ","executionInfo":{"status":"ok","timestamp":1733594475043,"user_tz":480,"elapsed":2,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"e438a14b","executionInfo":{"status":"ok","timestamp":1733594475420,"user_tz":480,"elapsed":379,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"outputs":[],"source":["data = pd.read_csv(data_url)"]},{"cell_type":"code","source":["# Clone the Tabula repository (optional, if you need additional resources)\n","!git clone https://github.com/zhao-zilong/Tabula.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ufcdiwxJ6sz","outputId":"84f2317e-240c-4058-f5c7-bc77fa897d3e","executionInfo":{"status":"ok","timestamp":1733594475420,"user_tz":480,"elapsed":4,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'Tabula' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","source":["# Install required packages\n","!pip install datasets>=2.5.2\n","!pip install numpy>=1.24.2\n","!pip install tqdm>=4.64.1\n","!pip install transformers>=4.22.1\n","!pip install pandas>=1.4.4\n","!pip install scikit_learn>=1.1.1\n","!pip install torch>=1.10.2"],"metadata":{"id":"DjVk6p_hJ6qe","executionInfo":{"status":"ok","timestamp":1733594523091,"user_tz":480,"elapsed":47673,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Add the Tabula directory to the Python path\n","import sys\n","sys.path.append('/content/Tabula')  # Adjust the path if necessary"],"metadata":{"id":"rqq2z0qOJ6oL","executionInfo":{"status":"ok","timestamp":1733594523091,"user_tz":480,"elapsed":2,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Import Necessary Libraries"],"metadata":{"id":"sipg7i2xxsw1"}},{"cell_type":"code","source":["# Import necessary libraries\n","import os\n","import warnings\n","import json\n","import typing as tp\n","import logging\n","import random\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn import preprocessing\n","from tqdm import tqdm\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n","    TrainingArguments,\n","    AutoConfig,\n","    DataCollatorWithPadding,\n","    Trainer\n",")\n","from datasets import Dataset\n","from dataclasses import dataclass"],"metadata":{"id":"zZKxJJn3J6l6","executionInfo":{"status":"ok","timestamp":1733594561307,"user_tz":480,"elapsed":38218,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### Load Dataset"],"metadata":{"id":"mouOA727xu-N"}},{"cell_type":"code","source":["# Load the dataset\n","data_url = \"https://raw.githubusercontent.com/zhao-zilong/Tabula/main/Real_Datasets/Insurance_compressed.csv\"\n","data = pd.read_csv(data_url)\n"],"metadata":{"id":"0Q4TIJiCJ6jd","executionInfo":{"status":"ok","timestamp":1733594561538,"user_tz":480,"elapsed":233,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Define Utility Functions"],"metadata":{"id":"ExSGZzwuxyPU"}},{"cell_type":"code","source":["# Define utility functions\n","def _array_to_dataframe(data: tp.Union[pd.DataFrame, np.ndarray], columns=None) -> pd.DataFrame:\n","    if isinstance(data, pd.DataFrame):\n","        return data\n","    assert isinstance(data, np.ndarray), \"Input needs to be a Pandas DataFrame or a Numpy NDArray\"\n","    assert columns, \"To convert the data into a Pandas DataFrame, a list of column names has to be given!\"\n","    assert len(columns) == len(data[0]), \\\n","        \"%d column names are given, but array has %d columns!\" % (len(columns), len(data[0]))\n","    return pd.DataFrame(data=data, columns=columns)\n","\n","def _get_column_distribution(df: pd.DataFrame, col: str) -> tp.Union[list, dict]:\n","    if df[col].dtype == \"float\" or df[col].dtype == \"int\":\n","        col_dist = df[col].to_list()\n","    else:\n","        col_dist = df[col].value_counts(normalize=True).to_dict()\n","    return col_dist\n","\n","def _convert_tokens_to_text(tokens: tp.List[torch.Tensor], tokenizer: AutoTokenizer) -> tp.List[str]:\n","    text_data = [tokenizer.decode(t, skip_special_tokens=True) for t in tokens]\n","    text_data = [d.replace(\"\\n\", \" \").replace(\"\\r\", \"\").strip() for d in text_data]\n","    return text_data\n","\n","def _convert_text_to_tabular_data(text: tp.List[str], df_gen: pd.DataFrame) -> pd.DataFrame:\n","    columns = df_gen.columns.to_list()\n","    result_list = []\n","    for t in text:\n","        features = t.split(\",\")\n","        td = dict.fromkeys(columns)\n","        for f in features:\n","            values = f.strip().split(\" \")\n","            if len(values) >= 2 and values[0] in columns and not td[values[0]]:\n","                td[values[0]] = [values[1]]\n","        result_list.append(pd.DataFrame(td))\n","    generated_df = pd.concat(result_list, ignore_index=True, axis=0)\n","    df_gen = pd.concat([df_gen, generated_df], ignore_index=True, axis=0)\n","    return df_gen\n","\n","def _pad(x, length: int, pad_value=50256):\n","    return [pad_value] * (length - len(x)) + x\n","\n","def _pad_tokens(tokens):\n","    max_length = len(max(tokens, key=len))\n","    tokens = [_pad(t, max_length) for t in tokens]\n","    return tokens\n","\n","def _seed_worker(_):\n","    worker_seed = torch.initial_seed() % 2**32\n","    random.seed(worker_seed)\n","    np.random.seed(worker_seed)\n","    torch.manual_seed(worker_seed)\n","    torch.cuda.manual_seed_all(worker_seed)\n","    # Shawn Chumbar Created this assignment\n"],"metadata":{"id":"1hCPwlzQJ6hY","executionInfo":{"status":"ok","timestamp":1733594561538,"user_tz":480,"elapsed":2,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["### Dfine TabulaStart Classes"],"metadata":{"id":"Z7c4FJzc1Uwu"}},{"cell_type":"code","source":["\n","# Define TabulaStart classes\n","class TabulaStart:\n","    def __init__(self, tokenizer):\n","        self.tokenizer = tokenizer\n","\n","    def get_start_tokens(self, n_samples: int) -> tp.List[tp.List[int]]:\n","        raise NotImplementedError(\"This has to be overwritten by the subclasses\")\n","\n","class CategoricalStart(TabulaStart):\n","    def __init__(self, tokenizer, start_col: str, start_col_dist: dict):\n","        super().__init__(tokenizer)\n","        assert isinstance(start_col, str), \"Start column name must be a string\"\n","        assert isinstance(start_col_dist, dict), \"Start column distribution must be a dict\"\n","        self.start_col = start_col\n","        self.population = list(start_col_dist.keys())\n","        self.weights = list(start_col_dist.values())\n","\n","    def get_start_tokens(self, n_samples):\n","        start_words = random.choices(self.population, self.weights, k=n_samples)\n","        start_text = [self.start_col + \" \" + str(s) + \",\" for s in start_words]\n","        start_tokens = _pad_tokens(self.tokenizer(start_text)[\"input_ids\"])\n","        return start_tokens\n","\n","class ContinuousStart(TabulaStart):\n","    def __init__(self, tokenizer, start_col: str, start_col_dist: tp.List[float],\n","                 noise: float = .01, decimal_places: int = 5):\n","        super().__init__(tokenizer)\n","        assert isinstance(start_col, str), \"Start column name must be a string\"\n","        assert isinstance(start_col_dist, list), \"Start column distribution must be a list\"\n","        self.start_col = start_col\n","        self.start_col_dist = start_col_dist\n","        self.noise = noise\n","        self.decimal_places = decimal_places\n","\n","    def get_start_tokens(self, n_samples):\n","        start_words = random.choices(self.start_col_dist, k=n_samples)\n","        start_words = [s + random.uniform(-self.noise, self.noise) for s in start_words]\n","        start_text = [self.start_col + \" \" + format(s, f\".{self.decimal_places}f\") + \",\" for s in start_words]\n","        start_tokens = _pad_tokens(self.tokenizer(start_text)[\"input_ids\"])\n","        return start_tokens\n","\n","class RandomStart(TabulaStart):\n","    def __init__(self, tokenizer, all_columns: tp.List[str]):\n","        super().__init__(tokenizer)\n","        self.all_columns = all_columns\n","\n","    def get_start_tokens(self, n_samples):\n","        start_words = random.choices(self.all_columns, k=n_samples)\n","        start_text = [s + \" \" for s in start_words]\n","        start_tokens = _pad_tokens(self.tokenizer(start_text)[\"input_ids\"])\n","        return start_tokens"],"metadata":{"id":"dfCtiBqYJ6ex","executionInfo":{"status":"ok","timestamp":1733594561729,"user_tz":480,"elapsed":192,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["### Define Tabula Dataset"],"metadata":{"id":"rZn2TASw1W-4"}},{"cell_type":"code","source":["\n","# Define TabulaDataset\n","class TabulaDataset(Dataset):\n","    def set_tokenizer(self, tokenizer):\n","        self.tokenizer = tokenizer\n","\n","    def _getitem(self, key: tp.Union[int, slice, str], decoded: bool = True, **kwargs) -> tp.Union[tp.Dict, tp.List]:\n","        row = self._data.fast_slice(key, 1)\n","        shuffle_idx = list(range(row.num_columns))\n","        random.shuffle(shuffle_idx)\n","        shuffled_text = \", \".join(\n","            [\"%s %s\" % (row.column_names[i], str(row.columns[i].to_pylist()[0]).strip()) for i in shuffle_idx]\n","        )\n","        tokenized_text = self.tokenizer(shuffled_text)\n","        return tokenized_text\n","\n","    def __getitems__(self, keys: tp.Union[int, slice, str, list]):\n","        if isinstance(keys, list):\n","            return [self._getitem(key) for key in keys]\n","        else:\n","            return self._getitem(keys)\n","\n","# Define Data Collator\n","@dataclass\n","class TabulaDataCollator(DataCollatorWithPadding):\n","    tokenizer: AutoTokenizer\n","\n","    def __call__(self, features: tp.List[tp.Dict[str, tp.Any]]):\n","        batch = self.tokenizer.pad(\n","            features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=self.return_tensors,\n","        )\n","        batch[\"labels\"] = batch[\"input_ids\"].clone()\n","        return batch\n"],"metadata":{"id":"HKDzOLh9KKtA","executionInfo":{"status":"ok","timestamp":1733594561729,"user_tz":480,"elapsed":3,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["### Define TabulaTrainer"],"metadata":{"id":"A7l6ajx81ZQG"}},{"cell_type":"code","source":["# Define TabulaTrainer\n","class TabulaTrainer(Trainer):\n","    def get_train_dataloader(self) -> DataLoader:\n","        if self.train_dataset is None:\n","            raise ValueError(\"Trainer: training requires a train_dataset.\")\n","\n","        data_collator = self.data_collator\n","        train_dataset = self.train_dataset  # Do not remove unused columns\n","\n","        train_sampler = self._get_train_sampler()\n","\n","        return DataLoader(\n","            train_dataset,\n","            batch_size=self._train_batch_size,\n","            sampler=train_sampler,\n","            collate_fn=data_collator,\n","            drop_last=self.args.dataloader_drop_last,\n","            num_workers=self.args.dataloader_num_workers,\n","            pin_memory=self.args.dataloader_pin_memory,\n","            worker_init_fn=_seed_worker,\n","        )\n"],"metadata":{"id":"ogXeZNFhKLZU","executionInfo":{"status":"ok","timestamp":1733594561729,"user_tz":480,"elapsed":3,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["### Define Main Tabula Class"],"metadata":{"id":"-hHBRgJX1bWw"}},{"cell_type":"code","source":["\n","# Define the main Tabula class\n","class Tabula:\n","    def __init__(self, llm: str, experiment_dir: str = \"trainer_tabula\", epochs: int = 100,\n","                 batch_size: int = 8, categorical_columns: list = [], **train_kwargs):\n","        self.llm = llm\n","        self.tokenizer = AutoTokenizer.from_pretrained(self.llm)\n","        self.tokenizer.pad_token = self.tokenizer.eos_token\n","        self.config = AutoConfig.from_pretrained(self.llm)\n","        self.model = AutoModelForCausalLM.from_pretrained(self.llm)\n","\n","        self.experiment_dir = experiment_dir\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.categorical_columns = categorical_columns\n","        self.train_hyperparameters = train_kwargs\n","\n","        self.columns = None\n","        self.num_cols = None\n","        self.conditional_col = None\n","        self.conditional_col_dist = None\n","        self.label_encoder_list = []\n","\n","    def encode_categorical_column(self, data: pd.DataFrame):\n","        self.label_encoder_list = []\n","        for column in data.columns:\n","            if column in self.categorical_columns:\n","                label_encoder = preprocessing.LabelEncoder()\n","                data[column] = data[column].astype(str)\n","                label_encoder.fit(data[column])\n","                data[column] = label_encoder.transform(data[column])\n","                self.label_encoder_list.append({\n","                    'column': column,\n","                    'label_encoder': label_encoder\n","                })\n","        return data\n","\n","    def decode_categorical_column(self, data: pd.DataFrame):\n","        for le_dict in self.label_encoder_list:\n","            column = le_dict['column']\n","            le = le_dict['label_encoder']\n","            allowed_values = list(range(len(le.classes_)))\n","            data[column] = pd.to_numeric(data[column], errors='coerce')\n","            data = data.dropna(subset=[column])\n","            data[column] = data[column].astype(int)\n","            data = data[data[column].isin(allowed_values)]\n","            data[column] = le.inverse_transform(data[column])\n","        return data\n","\n","    def fit(self, data: tp.Union[pd.DataFrame, np.ndarray], column_names: tp.Optional[tp.List[str]] = None,\n","            conditional_col: tp.Optional[str] = None, resume_from_checkpoint: tp.Union[bool, str] = False) \\\n","            -> TabulaTrainer:\n","        df = _array_to_dataframe(data, columns=column_names)\n","        self._update_column_information(df)\n","        self._update_conditional_information(df, conditional_col)\n","\n","        if self.categorical_columns:\n","            df = self.encode_categorical_column(df)\n","\n","        logging.info(\"Convert data into HuggingFace dataset object...\")\n","        tabula_ds = TabulaDataset.from_pandas(df)\n","        tabula_ds.set_tokenizer(self.tokenizer)\n","\n","        logging.info(\"Create Tabula Trainer...\")\n","        training_args = TrainingArguments(\n","            self.experiment_dir,\n","            num_train_epochs=self.epochs,\n","            per_device_train_batch_size=self.batch_size,\n","            save_strategy=\"no\",\n","            **self.train_hyperparameters\n","        )\n","        tabula_trainer = TabulaTrainer(\n","            self.model, training_args, train_dataset=tabula_ds, tokenizer=self.tokenizer,\n","            data_collator=TabulaDataCollator(tokenizer=self.tokenizer)\n","        )\n","\n","        logging.info(\"Start training...\")\n","        tabula_trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n","        return tabula_trainer\n","\n","    def sample(self, n_samples: int,\n","               start_col: tp.Optional[str] = \"\", start_col_dist: tp.Optional[tp.Union[dict, list]] = None,\n","               temperature: float = 0.7, k: int = 100, max_length: int = 100, device: str = \"cuda\") -> pd.DataFrame:\n","        tabula_start = self._get_start_sampler(start_col, start_col_dist)\n","\n","        self.model.to(device)\n","\n","        df_gen = pd.DataFrame(columns=self.columns)\n","\n","        with tqdm(total=n_samples) as pbar:\n","            already_generated = 0\n","            while n_samples > df_gen.shape[0]:\n","                start_tokens = tabula_start.get_start_tokens(k)\n","                start_tokens = torch.tensor(start_tokens).to(device)\n","\n","                tokens = self.model.generate(\n","                    input_ids=start_tokens,\n","                    max_length=max_length,\n","                    do_sample=True,\n","                    temperature=temperature,\n","                    pad_token_id=self.tokenizer.eos_token_id\n","                )\n","\n","                text_data = _convert_tokens_to_text(tokens, self.tokenizer)\n","                df_gen = _convert_text_to_tabular_data(text_data, df_gen)\n","\n","                for i_num_cols in self.num_cols:\n","                    df_gen = df_gen[pd.to_numeric(df_gen[i_num_cols], errors='coerce').notnull()]\n","\n","                df_gen[self.num_cols] = df_gen[self.num_cols].astype(float)\n","\n","                df_gen = df_gen.dropna(subset=self.columns)\n","\n","                pbar.update(df_gen.shape[0] - already_generated)\n","                already_generated = df_gen.shape[0]\n","\n","        df_gen = df_gen.reset_index(drop=True)\n","\n","        if self.categorical_columns:\n","            df_inversed = self.decode_categorical_column(df_gen.head(n_samples))\n","            return df_inversed\n","        else:\n","            return df_gen.head(n_samples)\n","\n","    def _update_column_information(self, df: pd.DataFrame):\n","        self.columns = df.columns.to_list()\n","        self.num_cols = df.select_dtypes(include=np.number).columns.to_list()\n","\n","    def _update_conditional_information(self, df: pd.DataFrame, conditional_col: tp.Optional[str] = None):\n","        assert conditional_col is None or conditional_col in df.columns, \\\n","            f\"The column name {conditional_col} is not in the feature names of the given dataset\"\n","\n","        self.conditional_col = conditional_col if conditional_col else df.columns[-1]\n","        self.conditional_col_dist = _get_column_distribution(df, self.conditional_col)\n","\n","    def _get_start_sampler(self, start_col: tp.Optional[str],\n","                           start_col_dist: tp.Optional[tp.Union[tp.Dict, tp.List]]) -> TabulaStart:\n","        if start_col and start_col_dist is None:\n","            raise ValueError(f\"Start column {start_col} was given, but no corresponding distribution.\")\n","        if start_col_dist is not None and not start_col:\n","            raise ValueError(f\"Start column distribution {start_col_dist} was given, the column name is missing.\")\n","\n","        start_col = start_col if start_col else self.conditional_col\n","        start_col_dist = start_col_dist if start_col_dist else self.conditional_col_dist\n","\n","        if isinstance(start_col_dist, dict):\n","            return CategoricalStart(self.tokenizer, start_col, start_col_dist)\n","        elif isinstance(start_col_dist, list):\n","            return ContinuousStart(self.tokenizer, start_col, start_col_dist)\n","        else:\n","            return RandomStart(self.tokenizer, self.columns)\n"],"metadata":{"id":"cJKf-o7YKLXI","executionInfo":{"status":"ok","timestamp":1733594561729,"user_tz":480,"elapsed":3,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["### Adjust Categorical Columns to Match Dataset"],"metadata":{"id":"3hPAhO7p1fI4"}},{"cell_type":"code","source":["# Adjust the categorical columns to match your dataset\n","categorical_columns = [\"sex\", \"children\", \"smoker\", \"region\"]"],"metadata":{"id":"UYtfhAnpKLVP","executionInfo":{"status":"ok","timestamp":1733594561729,"user_tz":480,"elapsed":2,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["### Instantiate Tabula Model with Correct Class Name and Parameters"],"metadata":{"id":"BUqk-YIk1h_0"}},{"cell_type":"code","source":["# Instantiate the Tabula model with the correct class name and parameters\n","model = Tabula(\n","    llm='distilgpt2',\n","    experiment_dir=\"insurance_training\",\n","    batch_size=32,\n","    epochs=4,\n","    categorical_columns=categorical_columns\n",")"],"metadata":{"id":"Q0N2AbUsKLUe","executionInfo":{"status":"ok","timestamp":1733594956540,"user_tz":480,"elapsed":906,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["model.fit(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":167},"id":"sIAaHcnqKLQY","outputId":"33d3ba44-80cf-40bd-83b9-a8c4bd727a38","executionInfo":{"status":"ok","timestamp":1733596567155,"user_tz":480,"elapsed":1600935,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-16-5b1c6e848dd2>:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `TabulaTrainer.__init__`. Use `processing_class` instead.\n","  tabula_trainer = TabulaTrainer(\n","You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='168' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [168/168 26:30, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["<__main__.TabulaTrainer at 0x7dffdcda4d60>"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["### Save Trained Model State"],"metadata":{"id":"JnAeuMXb1mHR"}},{"cell_type":"code","source":["# Save the trained model state\n","torch.save(model.model.state_dict(), \"insurance_training/model_400epoch.pt\")"],"metadata":{"id":"vLEPOBwrKT8e","executionInfo":{"status":"ok","timestamp":1733596827624,"user_tz":480,"elapsed":4760,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"qPbs3_Ura_06"}},{"cell_type":"markdown","source":["### Generate Synthetic Data"],"metadata":{"id":"pR7BC-iQ1n8e"}},{"cell_type":"code","source":["# Generate synthetic data\n","synthetic_data = model.sample(n_samples=1338)\n","synthetic_data.to_csv(\"insurance_400epoch.csv\", index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":347},"id":"9FVE_EM4KT5N","outputId":"31a79607-c1ef-4927-c563-c313a5882f6c","collapsed":true,"executionInfo":{"status":"error","timestamp":1733596812203,"user_tz":480,"elapsed":412,"user":{"displayName":"Dhruval Shah","userId":"16877617780739273734"}}},"execution_count":23,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-63af70656250>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate synthetic data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msynthetic_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1338\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msynthetic_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"insurance_400epoch.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-5b1c6e848dd2>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n_samples, start_col, start_col_dist, temperature, k, max_length, device)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtabula_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_start_sampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_col_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mdf_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3155\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3156\u001b[0m                 )\n\u001b[0;32m-> 3157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"]}]},{"cell_type":"code","source":[],"metadata":{"id":"84I6-X50bBFQ"},"execution_count":null,"outputs":[]}]}